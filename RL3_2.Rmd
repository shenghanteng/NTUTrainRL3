---
title: "衡量機器學習模型的表現"
author: "Yao-Jen Kuo"
date: "June, 2016"
output:
  slidy_presentation:
    fig_width: 8
    fig_height: 5
---

## 好的機器學習模型

一個好的機器學習模型大致上具有這幾個特性:

* 準確度高
* 計算時間短(2007 Netflix Prize)
* 可以適用到未知資料

## 基本機器學習模型的對應衡量指標

|機器學習模型|衡量表現指標|
|------------|------------|
|分類(Classification)|混淆矩陣Confusion Matrix|
|迴歸(Regression)|均方根誤差RMSE(Root Mean Squared Error)|
|                |R平方R-Squared|
|分群(Clustering)|WSS/TSS比例|
|                |唐恩指數Dunn's Index|

## 混淆矩陣Confusion Matrix

* 以二元分類器為例:

![Source: Google Search](images/confusionMatrix.png)

* 有很多的衡量指標由混淆矩陣衍生而來:

$$Accuracy = \frac{TP + TN}{TP + FP + FN + TN}$$

* 光使用 Accuracy 來判定會有什麼問題? 
* 假設有一個預測罕見疾病的分類器, 這個罕見疾病的比例大約是1%
* 我們的分類器很粗糙, 它一律猜測所有觀測值(病人)都沒有該疾病(陰性Negative):

|        |Positive|Negative|
|--------|--------|--------|
|Positive|0       |10      |
|Negative|0       |990     |

$$Accuracy = \frac{990 + 0}{990 + 10 + 0 + 0} = 0.99$$

* 因此關於混淆矩陣我們還有其他的指標:

$$TPR = \frac{TP}{TP + FN}$$

$$FPR = \frac{FP}{FP + TN}$$

* 還有其他的...但我們先緩一緩吧...

## 混淆矩陣Confusion Matrix - Hands On

* [資料集](https://www.kaggle.com/c/titanic/data)

```{r}
# 載入需要的套件
library(magrittr)
library(rpart)

# 讀取資料
titanic <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/train.csv", header = TRUE)
titanic <- titanic[, c(2, 3, 5, 6)] %>% na.omit
titanic$Survived <- factor(titanic$Survived, levels = c(1, 0))
str(titanic)

# 建立一個決策樹模型
set.seed(123)
treeModel <- rpart(Survived ~ ., data = titanic, method = "class")
titanicToBePredicted <- titanic[, -1]
prediction <- predict(treeModel, newdata = titanicToBePredicted, type = "class")

# 將混淆矩陣印出來
confusionMatrix <- table(titanic$Survived, prediction, dnn = c("Actual", "Predicted"))
confusionMatrix

# 獲得TP, TN, FP, FN
TP <- confusionMatrix[1, 1]
TN <- confusionMatrix[2, 2]
FP <- confusionMatrix[2, 1]
FN <- confusionMatrix[1, 2]

# 計算accuracy, TPR, FPR
accuracy <- (TP + TN)/(TP + TN + FP + FN)
accuracy <- sum(diag(confusionMatrix))/sum(confusionMatrix)# 試試這樣算
TPR <- TP/(TP + FN)
FPR <- FP/(FP + TN)

# accuracy, TPR, FPR
accuracy
TPR
FPR
```

## 混淆矩陣Confusion Matrix - Do It Yourself

* 這次換你囉, 加入`Fare`, `Cabin`, 與`Embarked`這三個變數試試看!
* 印出混淆矩陣
* 計算 accuracy, TPR與FPR

```{r}
# 讀取資料
titanic <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/train.csv", header = TRUE)
titanic <- titanic[, c(2, 3, 5, 6, 10:12)] %>% na.omit
titanic$Survived <- factor(titanic$Survived, levels = c(1, 0))
str(titanic)

# 建立一個決策樹模型
set.seed(123)
treeModel <- rpart(Survived ~ ., data = titanic, method = "class")
titanicToBePredicted <- titanic[, -1]
prediction <- predict(treeModel, newdata = titanicToBePredicted, type = "class")

# 將混淆矩陣印出來
confusionMatrix <- table(titanic$Survived, prediction, dnn = c("Actual", "Predicted"))
confusionMatrix

# 獲得TP, TN, FP, FN
TP <- confusionMatrix[1, 1]
TN <- confusionMatrix[2, 2]
FP <- confusionMatrix[2, 1]
FN <- confusionMatrix[1, 2]

# 計算accuracy, TPR, FPR
accuracy <- (TP + TN)/(TP + TN + FP + FN)
accuracy <- sum(diag(confusionMatrix))/sum(confusionMatrix)# 試試這樣算
TPR <- TP/(TP + FN)
FPR <- FP/(FP + TN)

# accuracy, TPR, FPR
accuracy
TPR
FPR
```

## 均方根誤差RMSE

* 資料點與預測迴歸線的平均距離

$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2}$$

![Source: Google Search](images/rmse.jpg)

## 均方根誤差RMSE - Hands on

```{r}
# 氣溫與冰紅茶銷量
temperature <- c(29, 28, 34, 31, 25, 29, 32, 31, 24, 33, 25, 31, 26, 30)
icedTeaSales <- c(77, 62, 93, 84, 59, 64, 80, 75, 58, 91, 51, 73, 65, 84)
icedTeaData <- data.frame(temperature, icedTeaSales)

# 建立一個線性迴歸模型
lmFit <- lm(formula = icedTeaSales ~ temperature, data = icedTeaData)
icedTeaDataToBePredicted <- data.frame(icedTeaData[, -2])
prediction <- predict(lmFit, newdata = icedTeaDataToBePredicted)
RMSE <- sqrt(sum( (icedTeaData$icedTeaSales - prediction) ^ 2) / nrow(icedTeaData))
RMSE
```

## 均方根誤差RMSE - Do It Yourself(1)

* 來這裡下載資料集[airUCI](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise#)
* 使用常用的函數暸解 `airUCI` 的結構
* 只使用`freq`, `angle`, 以及`chLength`這三個變數來預測`dec`
* 使用 `+` 在 `formula` 連接多個變數
* 計算RMSE1

```{r}
# 直接從UCI讀入資料集
airUCI <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat", header = FALSE, col.names = c("freq", "angle", "chLength", "velocity", "thickness", "dec"))
# 建立一個線性模型lmFit_1, 用練習題規定的三個變數預測dec
lmFit_1 <- lm(formula = dec ~ freq + angle + chLength, data = airUCI)
# 將原始資料的三個變數篩選出
airUCIToBePredicted <- data.frame(airUCI[, c(1:3)])
# 用建好的模型預測dec
prediction <- predict(lmFit_1, newdata = airUCIToBePredicted)
# 再利用公式計算預測趨勢線與實際值的的RMSE
RMSE1 <- sqrt(sum( (airUCI$dec - prediction) ^ 2) / nrow(airUCI))
RMSE1
```

## 均方根誤差RMSE - Do It Yourself(2)

* 再加入`velocity`以及`thickness`這兩個變數, 共5個變數來預測`dec`
* 計算RMSE2
* 比較RMSE1與RMSE2

```{r}
# 建立一個線性模型lmFit_2, 用規定的五個變數預測dec
lmFit_2 <- lm(formula = dec ~ freq + angle + chLength + velocity + thickness, data = airUCI)
# 將原始資料的五個變數篩選出
airUCIToBePredicted <- data.frame(airUCI[, c(1:5)])
# 用建好的模型預測dec
prediction <- predict(lmFit_2, newdata = airUCIToBePredicted)
# 再利用公式計算預測趨勢線與實際值的的RMSE
RMSE2 <- sqrt(sum( (airUCI$dec - prediction) ^ 2) / nrow(airUCI))

# 比較 RMSE1 與 RMSE2
RMSE2 - RMSE1
```

## R平方R-Squared

* y變數的變異有多少比例可以被x變數預測

$$SS_{res} = \sum_{i=1}^N(y_i - \hat{y}_i) ^ 2$$
$$SS_{tot} = \sum_{i=1}^N(y_i - \bar{y}) ^ 2$$
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

## R平方R-Squared - Hands on

```{r}
icedTeaSalesEst <- predict(lmFit, newdata = data.frame(icedTeaData[, 1]))

# 計算R-squared
res <- icedTeaData$icedTeaSales - icedTeaSalesEst
ss_res <- sum (res ^ 2)
ss_tot <- sum( (icedTeaData$icedTeaSales - mean(icedTeaData$icedTeaSales))^2 )
r_sq <- 1 - ss_res / ss_tot
r_sq

# 跟 summary() 的結果比較
summary(lmFit)$r.squared
```

## R平方R-Squared - Do It Yourself

* 延續使用資料集[airUCI](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise#)
* 只使用`freq`, `angle`, 以及`chLength`這三個變數來預測`dec`
* 自行計算R平方
* 與 `summary()` 的結果比較

```{r}
lmFit <- lm(formula = dec ~ freq + angle + chLength, data = airUCI)

decEst <- predict(lmFit, newdata = data.frame(airUCI[, 1:3]))

# 計算R-squared
res <- airUCI$dec - decEst
ss_res <- sum (res ^ 2)
ss_tot <- sum( (airUCI$dec - mean(airUCI$dec))^2 )
r_sq <- 1 - ss_res / ss_tot
r_sq

# 跟 summary() 的結果比較
summary(lmFit)$r.squared
```

## WSS / TSS 比例

* 同一個群集中的**相似度**要高
* 不同群集之間的**相似度**要低
* 我們先使用**WSS / TSS**來判別分群模型的表現

* 組內距離平方和WSS(Within Cluster Sum of Squares)

$$Min.\,WSS = \sum_{i=1}^{N_C}\sum_{x \in C_i}d(x,\,\bar{x}_{C_i})^2$$

* 組間距離平方和BSS(Between Cluster Sum of Squares)

$$Max.\,BSS = \sum_{i=1}^{N_C}\lvert C_i \rvert \cdot d(\bar{x}_{C_i},\,\bar{x})^2$$

* 總離均差平方和TSS(Total Cluster Sum of Squares)

$$TSS = WSS + BSS$$

## WSS / TSS 比例 - Hands on

```{r}
# 看看鳶尾花資料集的結構
str(iris)

# 建立一個分群模型
irisCluster <- iris[-5]
kmeansIris<-kmeans(irisCluster, centers = 3)

# 作圖
plot(formula = Petal.Length ~ Petal.Width, data = irisCluster, col = kmeansIris$cluster, main = "將鳶尾花做分群", xlab = "花瓣寬度", ylab = "花瓣長度", family = "STHeiti")

# 計算WSS/TSS的比例
WSS <- kmeansIris$tot.withinss
BSS <- kmeansIris$betweenss
ratio <- WSS/(WSS + BSS)
ratio
```

## WSS / TSS 比例 - Do It Yourself

* 來這裡下載資料集[seedsUCI](https://archive.ics.uci.edu/ml/datasets/seeds)
* 使用常用函數了解資料框
* 仿照前例, 計算WSS/TSS

```{r}
seedsUCI <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/seedsUCI.csv", header = FALSE, col.names = c("area", "perimeter", "compactness", "length", "width", "asymmetry", "grooveLength", "type"))
str(seedsUCI)
seedsUCIToBeClustered <- seedsUCI[, -8]
kmeansSeeds<-kmeans(seedsUCIToBeClustered, centers = 3)

# 計算WSS/TSS的比例
WSS <- kmeansSeeds$tot.withinss
BSS <- kmeansSeeds$betweenss
ratio <- WSS/(WSS + BSS)
ratio
```

## 關於機器學習, 資料探勘與統計

* 機器學習對於**未知資料**的預測較為在意
* 資料探勘對於**現有資料**的樣態或找出有趣因子更為在意
* 統計對於模型與資料的配適度更為在意
* 資料探勘跟機器學習其實相輔相成，實務上你很難區隔
* 統計是實現機器學習的方法之一

## 訓練與測試

* 實務上我們要將資料分為**訓練(training)**與**測試(test)**
* 這兩組資料不重疊
* 建議比例為70%與30%
* 在監督式學習(分類與迴歸)使用，因為非監督式學習(分群)資料並沒有答案

## 怎麼切分你的訓練與測試

* 如果是做分類，類型的分佈在訓練與測試資料中的分佈應該要相似
* 不要遺漏任何一種類別
* 把資料做洗牌以後再做切分

![Source: Google Search](images/training_test_sets.png)

## 怎麼切分你的訓練與測試 - Hands on

* 洗牌我們可以善用`sample()`函數，查詢一下`sample()`函數吧！

```{r}
titanic <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/train.csv", header = TRUE)
titanic <- titanic %>% na.omit

# 洗牌
n <- nrow(titanic)
set.seed(123)
shuffledTitanic <- titanic[sample(n), ]

# 看看洗牌前與洗牌後的titanic
head(titanic)
head(shuffledTitanic)

# 依照70%:30%的比例切分training與test
trainingIndices <- 1:round(0.7 * n)
training <- shuffledTitanic[trainingIndices, ]
testIndices <- (round(0.7 * n) + 1):n
test <- shuffledTitanic[testIndices, ]

# 印出training與test的結構
str(training)
str(test)
```

## 怎麼切分你的訓練與測試 - Do It Yourself

* 我希望training約有536個觀測值, test約有178個觀測值

```{r}
# 536 / 714 = 0.75
# 依照75%:25%的比例切分training與test
trainingIndices <- 1:round(0.75 * n)
training <- shuffledTitanic[trainingIndices, ]
testIndices <- (round(0.75 * n) + 1):n
test <- shuffledTitanic[testIndices, ]
```

## 更細膩精準地切分你的訓練與測試

* 在前面的例子我們只使用一次`sample()`函數
* 更細膩的方法稱為**交叉驗證(cross validation)**
* 最後以平均的指標(可能是accuracy或TPR...etc.)作為結果

![Source: Google Search](images/cross_validation.png)

## 更細膩精準地切分你的訓練與測試 - Hands on

* 用`Pclass`, `Sex`, 與`Age`作為預測變數就好
* k-fold cross validation, k = 4

```{r}
titanic <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/train.csv", header = TRUE)
titanic <- titanic[, c("Survived", "Pclass", "Sex", "Age")]
titanic <- titanic %>% na.omit
titanic$Survived <- as.factor(titanic$Survived)
titanic$Pclass <- as.factor(titanic$Pclass)

# 洗牌
n <- nrow(titanic)
set.seed(123)
shuffledTitanic <- titanic[sample(n), ]

# accuracies向量
accuracies <- rep(NA, times = 4)

for (i in 1:4) {
  # test的列數
  indices <- (((i-1) * round((1/4)*nrow(shuffledTitanic))) + 1):((i*round((1/4) * nrow(shuffledTitanic))))
  training <- shuffledTitanic[-indices,]
  test <- shuffledTitanic[indices,]
  # 設定亂數種子
  set.seed(123)
  # 建立決策樹模型
  tree <- rpart(Survived ~ ., training, method = "class")
  
  # 預測
  prediction <- predict(tree, test, type="class")
  
  # 建立混淆矩陣
  confusionMatrix <- table(test$Survived, prediction)
  
  # 把每次的accuracy放回accuracies向量
  accuracies[i] <- sum(diag(confusionMatrix))/(sum(confusionMatrix))
}
# 平均這4個accuracy
mean(accuracies)
```

## 更細膩精準地切分你的訓練與測試 - Do It Yourself

* k-fold cross validation, k = 8
* 將平均的 accuracies 印出

```{r}
# 洗牌
n <- nrow(titanic)
set.seed(123)
shuffledTitanic <- titanic[sample(n), ]

# accuracies向量
accuracies <- rep(NA, times = 8)
kFold <- length(accuracies)

for (i in 1:kFold) {
  # test的列數
  indices <- (((i-1) * round((1/kFold)*nrow(shuffledTitanic))) + 1):((i*round((1/kFold) * nrow(shuffledTitanic))))
  training <- shuffledTitanic[-indices,]
  test <- shuffledTitanic[indices,]
  # 設定亂數種子
  set.seed(123)
  # 建立決策樹模型
  tree <- rpart(Survived ~ ., training, method = "class")
  
  # 預測
  prediction <- predict(tree, test, type="class")
  
  # 建立混淆矩陣
  confusionMatrix <- table(test$Survived, prediction)
  
  # 把每次的accuracy放回accuracies向量
  accuracies[i] <- sum(diag(confusionMatrix))/(sum(confusionMatrix))
}
# 平均這8個accuracy
mean(accuracies)
```

## Underfitting vs. Overfitting?

* 你是否常常聽到人家說underfitting或overfitting?

![Source: Google Search](images/fittingDiagram_1.png)

![Source: Google Search](images/fittingDiagram_2.png)

* 在**適用性**與**精確性**之間取得良好的平衡

## Underfitting vs. Overfitting? - Hands on

* 我們的分類器overfitting(對訓練資料集的分類完全正確, accuracy = 100%)
* 但是對測試資料集的分類accuracy僅有62.5%

```{r}
# 倒帳資料集
debtIncomeRatio <- c(1, 2.112, 4.123, 1.863, 2.973, 1.687, 5.891, 3.167, 1.23, 2.441, 3.555, 3.25, 1.333)
default <- c(0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1)
defaultDataTrain <- data.frame(debtIncomeRatio, default)
defaultDataTrain$default <- factor(defaultDataTrain$default)

# 分類器
default_classifier <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 4] <- 1
  prediction[x >= 3 & x <= 4] <- 0
  prediction[x >= 2.2 & x < 3] <- 1
  prediction[x >= 1.4 & x < 2.2] <- 0
  prediction[x > 1.25 & x < 1.4] <- 1
  prediction[x <= 1.25] <- 0
  return(prediction)
}

# training資料集的accuracy
defaultPredictionOverTrain <- default_classifier(defaultDataTrain$debtIncomeRatio)
confusionMatrixOverTrain <- table(defaultDataTrain$default, defaultPredictionOverTrain)
accuracyOverTrain <- sum(diag(confusionMatrixOverTrain)) / sum(confusionMatrixOverTrain)
accuracyOverTrain

# test資料集
debtIncomeRatio <- c(1.5, 	4.941, 	3.429, 	3.493, 	3.38, 	3.689, 	1, 	6.761, 	2.195, 	2.857, 	1.883, 	1.744, 	2.325, 	1.25, 	1.8, 	2.5, 	4.617, 	2.206, 	8.323, 	3.764, 	1.932, 	3.153, 	1.983, 	3.812, 	4.295, 	4.5, 	1.3, 	1, 	5.009, 	5.888, 	1, 	3.456, 	3.379, 	1.755, 	2.333, 	2.188, 	1.406, 	1.857, 	3.707, 	3.766, 	2.93, 	2.076, 	1.9, 	1.684, 	2.769, 	4.571, 	3.029, 	3.628)
default <- c(0, 	1, 	1, 	1, 	0, 	1, 	0, 	1, 	1, 	0, 	0, 	0, 	0, 	0, 	1, 	0, 	1, 	0, 	1, 	0, 	0, 	0, 	0, 	1, 	0, 	1, 	0, 	0, 	1, 	0, 	0, 	1, 	0, 	0, 	0, 	0, 	0, 	0, 	1, 	0, 	1, 	0, 	0, 	1, 	1, 	0, 	0, 	0)
defaultDataTest <- data.frame(debtIncomeRatio, default)
defaultDataTest$default <- as.factor(defaultDataTest$default)

# test資料集的accuracy
defaultPredictionOverTest <- default_classifier(defaultDataTest$debtIncomeRatio)
confusionMatrixOverTest <- table(defaultDataTest$default, defaultPredictionOverTest)
accuracyOverTest <- sum(diag(confusionMatrixOverTest)) / sum(confusionMatrixOverTest)
accuracyOverTest
```

## Underfitting vs. Overfitting? - Do It Yourself

* 用新的分類器對**訓練**資料分類並計算accuracy
* 用新的分類器對**測試**資料分類並計算accuracy
* 跟前面的例子比較一下

```{r}
# 新的分類器
default_classifier <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 4] <- 1
  prediction[x <= 4] <- 0
  return(prediction)
}

# training資料集的accuracy
defaultPredictionTrain <- default_classifier(defaultDataTrain$debtIncomeRatio)
confusionMatrixTrain <- table(defaultDataTrain$default, defaultPredictionTrain)
accuracyTrain <- sum(diag(confusionMatrixTrain)) / sum(confusionMatrixTrain)
accuracyTrain

# test資料集的accuracy
defaultPredictionTest <- default_classifier(defaultDataTest$debtIncomeRatio)
confusionMatrixTest <- table(defaultDataTest$default, defaultPredictionTest)
accuracyTest <- sum(diag(confusionMatrixTest)) / sum(confusionMatrixTest)
accuracyTest
```